<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <style>
      blockquote {
        padding-left: 1em;
        border-left: 1px solid;
      }
    </style>

    <title>
      
        PACT 2022 — October 10–12, 2022
      
    </title>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
  </head>
  <body>
    <div class="container">
      <header class="d-flex flex-wrap justify-content-center py-3 mb-4 border-bottom">
        <a href="./index.html" class="d-flex align-items-center mb-3 mb-md-0 me-md-auto text-dark text-decoration-none">
          <img class="bi me-2" height="32" src="images/pact-logo.png">
          <span class="fs-4">PACT 2022</span>
          <span class="fs-6 text-muted">  October 10–12, 2022</span>
        </a>

        <ul class="nav nav-pills">
          

          
          <li class="nav-item">
            <a href="./index.html"
              
                class="nav-link"
              
              >Home</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./submit.html"
              
                class="nav-link"
              
              >Submit</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./attend.html"
              
                class="nav-link"
              
              >Attend</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./program.html"
              
                class="nav-link active" aria-current="page"
              
              >Program</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./tutorials.html"
              
                class="nav-link"
              
              >Workshops/Tutorials</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./src.html"
              
                class="nav-link"
              
              >ACM SRC</a>
          </li>
          
          
          <li class="nav-item">
            <a href="./organization.html"
              
                class="nav-link"
              
              >Organization</a>
          </li>
          
        </ul>
      </header>
      <main>
        
        
        
  <div class="row mb-3">
    <div class="col-md-8">
      
    <h1> Main Conference Program <a id="top"></a></h1>

    

    
    <ul>
    <li><a href="tutorials.html#tut0">October 8: Tutorials and Workshops Day 1</a></li>
    <li><a href="tutorials.html#tut1">October 9: Tutorials and Workshops Day 2</a></li>
    <li><a href="program.html#main0">October 10: Main Conference Day 1</a></li>
    <li><a href="program.html#main1">October 11: Main Conference Day 2</a></li>
    <li>
    <p><a href="program.html#main2">October 12: Main Conference Day 3</a></p>
    </li>
    <li>
    <p><strong>DPI</strong> below refers to the <strong>Discovery Partners Institute</strong>, on
      the fourth floor of 200 S. Wacker Drive, Chicago, IL.</p>
    </li>
    <li><strong>IC</strong> below refers to the <strong>Illini Center</strong>, on the 19th
      floor of 200 S. Wacker Drive, Chicago, IL.</li>
    </ul>


    <a id="main0"></a>

    <h2>Monday, October 10, 2022</h2>

    <a href="#top">Back to navigation</a>

    
  <table class="table table-striped">
    <thead>
      <td>Time</td>
      <td>What</td>
      <td>Where</td>
    </thead>
    
        
      <tr>
        <td>7:30</td>
        <td><p>Registration opens</p>

              <p>
              Please allow sufficient time to clear building security.
              See <a href='attend.html#arrival'>here</a> for instructions.
              </p>
              </td>
        <td>DPI</td>
      </tr>


    
  
      <tr>
        <td>7:30–8:20</td>
        <td>Continental Breakfast</td>
        <td>Discovery Room, DPI</td>
      </tr>


    <tr>
      <td>8:20–8:30</td>
      <td>Welcome from the Chairs</td>
      <td>Discovery Room, DPI</td>
    </tr>
    
  <tr>
    <td>8:30–9:30</td>
    <td>Keynote: <b><a href="#keynote-0">Closing the Gap between Quantum Algorithms and Machines with Hardware-Software Co-Design</a></b></td>
    <td>Discovery Room, DPI</td>
  </tr>

    
  
      <tr>
        <td>9:30–10:00</td>
        <td>Coffee Break</td>
        <td>Discovery Room, DPI</td>
      </tr>


    
  
    <tr>
      <td>10:00–12:00</td>
      <td>
  <p><b>Track 1: Compilers for ever</b></p>
  
    <p><i>Session Chair: Nelson Amaral</i></p>
  
  <ul>
    
      <li>
        10:00–10:30:
        <b>ReACT: Redundancy-Aware Code Generation for Tensor Expressions  (#295)</b>

        <em>T. Zhou, R. Tian, R. Ashraf, R. Gioiosa, G. Kestor, V. Sarkar</em>
      </li>
    
      <li>
        10:30–11:00:
        <b>Com-CAS: Effective Cache Apportioning Under Compiler Guidance  (#12)</b>

        <em>B. Chatterjee, S. Khan, S. Pande</em>
      </li>
    
      <li>
        11:00–11:30:
        <b>Transfer-Tuning: Reusing Auto-Schedules for Efficient Tensor Program Code Generation  (#267)</b>

        <em>P. Gibson, J. Cano</em>
      </li>
    
      <li>
        11:30–12:00:
        <b>HBMax: Optimizing Memory Efficiency for Parallel Influence Maximization on Multicore Architectures  (#26)</b>

        <em>X. Chen, M. Minutoli, J. Tian, M. Halappanavar, A. Kalyanaraman, D. Tao</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>10:00–12:00</td>
      <td>
  <p><b>Track 2: Optimizing the execution of GNNs</b></p>
  
    <p><i>Session Chair: Antonino Tumeo</i></p>
  
  <ul>
    
      <li>
        10:00–10:30:
        <b>Slice-and-Forge: Making Better Use of Caches for Graph Convolutional Network Accelerators  (#520)</b>

        <em>M. Yoo, J. Song, H. Lee, J. Lee, N. Kim, Y. Kim, J. Lee</em>
      </li>
    
      <li>
        10:30–11:00:
        <b>GNNear: Accelerating Full-Batch Training of Graph Neural Networks with Near-Memory Processing  (#148)</b>

        <em>Z. Zhou, C. Li, X. Wei, X. Wang, G. Sun</em>
      </li>
    
      <li>
        11:00–11:30:
        <b>T-GCN: A Sampling Based Streaming Graph Neural Network System With Hybrid Architecture  (#31)</b>

        <em>C. Huan, S. Song, Y. Liu, H. Zhang, H. Liu, C. He, K. Chen, J. Jiang, Y. Wu</em>
      </li>
    
      <li>
        11:30–12:00:
        <b>Optimizing Aggregate Computation of Graph Neural Networks with On-GPU Interpreter-Style Programming  (#403)</b>

        <em>Z. Ji, C. Wang</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    
  
      <tr>
        <td>12:00–13:30</td>
        <td>Lunch</td>
        <td><a href='attend.html#restaurants'>(Attendees on their own)</a></td>
      </tr>


    <tr>
      <td>12:30–13:30</td>
      <td>Steering Committee Meeting</td>
      <td>Illini Room, IC</td>
    </tr>
    
  
    <tr>
      <td>13:30–15:00</td>
      <td>
  <p><b>Track 1: Getting more out of your memory</b></p>
  
    <p><i>Session Chair: Jose Moreira</i></p>
  
  <ul>
    
      <li>
        13:30–14:00:
        <b>FlatPack: Flexible Compaction of Compressed Memory  (#66)</b>

        <em>A. Eldstål-Ahrens, A. Arelakis, I. Sourdis</em>
      </li>
    
      <li>
        14:00–14:30:
        <b>Pavise: Integrating Fault Tolerance Support for Persistent Memory Applications  (#118)</b>

        <em>H. Qiu, S. Liu, X. Song, S. Khan, G. Pekhimenko</em>
      </li>
    
      <li>
        14:30–15:00:
        <b>Efficient Atomic Durability on eADR-enabled Persistent Memory  (#199)</b>

        <em>T. Zhou, Y. Du, F. Yang, X. Liao, Y. Lu</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>13:30–15:00</td>
      <td>
  <p><b>Track 2: Sparse matrix computations</b></p>
  
    <p><i>Session Chair: Gagan Agrawal</i></p>
  
  <ul>
    
      <li>
        13:30–14:00:
        <b>Probing the Efficacy of Hardware-Aware Weight Pruning to Optimize the SpMM routine on Ampere GPUs  (#416)</b>

        <em>R. Castro, D. Andrade, B. Fraguela</em>
      </li>
    
      <li>
        14:00–14:30:
        <b>Squaring the circle: Executing Sparse Matrix Computations on FlexTPU—a TPU-like processor  (#133)</b>

        <em>X. He, K. Chen, S. Feng, H. Kim, D. Blaauw, R. Dreslinski, T. Mudge</em>
      </li>
    
      <li>
        14:30–15:00:
        <b>Custom High-Performance Vector Code Generation for Data-Specific Sparse Computations  (#139)</b>

        <em>M. Horro, L. Pouchet, G. Rodríguez, J. Tourino</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    
  
      <tr>
        <td>15:00–15:30</td>
        <td>Coffee Break</td>
        <td>Discovery Room, DPI</td>
      </tr>


    
  
    <tr>
      <td>15:30–17:00</td>
      <td>
  <p><b>Track 1: Graph processing</b></p>
  
    <p><i>Session Chair: Vivek Sarkar</i></p>
  
  <ul>
    
      <li>
        15:30–16:00:
        <b>Batched Graph Community Detection on GPUs  (#72)</b>

        <em>H. Chou, S. Ghosh</em>
      </li>
    
      <li>
        16:00–16:30:
        <b>SampleMine: A Framework for Applying Random Sampling to Subgraph Pattern Mining through Loop Perforation  (#85)</b>

        <em>P. Jiang, Y. Wei, J. Su, R. Wang, B. Wu</em>
      </li>
    
      <li>
        16:30–17:00:
        <b>Decoupling Scheduler, Topology Layout, and Algorithm to Easily Enlarge the Tuning Space of GPU Graph Processing  (#308)</b>

        <em>S. Jeong, Y. Lee, J. Lee, H. Choi, S. Song, J. Lee, Y. Kim, H. Kim</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>15:30–17:00</td>
      <td>
  <p><b>Track 2: Miscellaneous</b></p>
  
    <p><i>Session Chair: Jose Moreira</i></p>
  
  <ul>
    
      <li>
        15:30–16:00:
        <b>Tiered Hashing: Revamping Hash Indexing under a Unified Memory-Storage Hierarchy  (#58)</b>

        <em>J. Zhou, J. Wu, W. Huang, Y. Zhou, F. Wu, L. Shi, X. Zhang, K. Wang, F. Zhu, S. Li, W. Wang</em>
      </li>
    
      <li>
        16:00–16:30:
        <b>Understanding and Reaching the Performance Limit of Schedule Tuning on Stable Synchronization Determinism  (#145)</b>

        <em>Q. Zhao, Z. Qiu, S. Shao, X. Hui, H. Khan, G. Jin</em>
      </li>
    
      <li>
        16:30–17:00:
        <b>VoxelCache: Accelerating Online Mapping in Robotics and 3D Reconstruction Tasks  (#183)</b>

        <em>S. Durvasula, R. Kiguru, S. Mathur, J. Xu, J. Lin, N. Vijaykumar</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    
  
      <tr>
        <td>17:00–19:00</td>
        <td><b>Poster Session</b> / Reception</td>
        <td>Classroom B, DPI</td>
      </tr>


  </table>


    <a id="keynote-0"></a>

    <h3>Keynote: Closing the Gap between Quantum Algorithms and Machines with Hardware-Software Co-Design</h3>
<p><strong>Fred Chong</strong> (Department of Computer Science, University of Chicago, Chicago, IL)</p>
<p>Quantum computing is at an inflection point, where 127-qubit
machines are deployed, and 1000-qubit machines are perhaps only a
few years away.  These machines have the potential to fundamentally
change our concept of what is computable and demonstrate practical
applications in areas such as quantum chemistry, optimization,
and quantum simulation.  Yet a significant resource gap remains
between practical quantum algorithms and real machines.  A promising
approach to closing this gap is to design software that is aware
of the key physical properties of emerging quantum technologies.
I will illustrate this approach with some of our recent work that
focuses on techniques that break traditional abstractions and
inform hardware design, including compiling programs directly
to analog control pulses,  computing with ternary quantum bits,
2.5D architectures for surface codes, and exploiting long-distance
communication and tolerating atom loss in neutral-atom machines.</p>
<blockquote>
<p><img alt="Fred Chong" src="images-generated/headshot-chong.jpg" /></p>
<p>Fred Chong is the Seymour Goodman Professor in the Department of
Computer Science at the University of Chicago and the Chief Scientist
for Quantum Software at ColdQuanta. He is also Lead Principal
Investigator for the EPiQC Project (Enabling Practical-scale Quantum
Computing), an NSF Expedition in Computing. Chong is a member of the
National Quantum Advisory Committee (NQIAC) which provides advice
to the President and Secretary of Energy on the National Quantum
Initiative Program. In 2020, he co-founded Super.tech, a quantum
software company, which was acquired by ColdQuanta in 2022. Chong
received his Ph.D. from MIT in 1996 and was a faculty member and
Chancellor's fellow at UC Davis from 1997-2005. He was also a
Professor of Computer Science, Director of Computer Engineering,
and Director of the Greenscale Center for Energy-Efficient Computing
at UCSB from 2005-2015. He is a recipient of the NSF CAREER award,
the Intel Outstanding Researcher Award, and 13 best paper awards.</p>
</blockquote>

    <a id="main1"></a>

    <h2>Tuesday, October 11, 2022</h2>

    <a href="#top">Back to navigation</a>

    
  <table class="table table-striped">
    <thead>
      <td>Time</td>
      <td>What</td>
      <td>Where</td>
    </thead>
    
        
      <tr>
        <td>7:30</td>
        <td><p>Registration opens</p>

              <p>
              Please allow sufficient time to clear building security.
              See <a href='attend.html#arrival'>here</a> for instructions.
              </p>
              </td>
        <td>DPI</td>
      </tr>


    
  
      <tr>
        <td>7:30–8:25</td>
        <td>Continental Breakfast</td>
        <td>Discovery Room, DPI</td>
      </tr>


    <tr>
      <td>8:25–8:30</td>
      <td>PACT 2023 in Vienna: A Preview</td>
      <td>Discovery Room, DPI</td>
    </tr>
    
  <tr>
    <td>8:30–9:30</td>
    <td>Keynote: <b><a href="#keynote-1">MemComputing: Fundamentals and Applications</a></b></td>
    <td>Discovery Room, DPI</td>
  </tr>

    
  
      <tr>
        <td>9:30–10:00</td>
        <td>Coffee Break</td>
        <td>Discovery Room, DPI</td>
      </tr>


    <tr>
      <td>10:00–12:00</td>
      <td><b>ACM SRC Poster Session</b></td>
      <td>Discovery Room, DPI</td>
    </tr>
    
  
    <tr>
      <td>10:00–12:00</td>
      <td>
  <p><b>Track 1: Better neural networks</b></p>
  
    <p><i>Session Chair: Jose Cano Reyes</i></p>
  
  <ul>
    
      <li>
        10:00–10:30:
        <b>Effective Performance Modeling and Domain-Specific Compiler Optimization of CNNs for GPUs  (#178)</b>

        <em>Y. Xu, Q. Yuan, E. Barton, R. Li, P. Sadayappan, A. Sukumaran-Rajam</em>
      </li>
    
      <li>
        10:30–11:00:
        <b>High-performance Architecture Aware Sparse Convolutional Neural Networks for GPUs  (#136)</b>

        <em>L. Xiang, P. Sadayappan, A. Sukumaran-Rajam</em>
      </li>
    
      <li>
        11:00–11:30:
        <b>Weightless Neural Networks for Efficient Edge Inference  (#256)</b>

        <em>Z. Susskind, A. Arora, I. Miranda, L. Villon, R. Katopodis, L. de Araújo, D. Dutra, P. Lima, F. França, M. Breternitz Jr., L. John</em>
      </li>
    
      <li>
        11:30–12:00:
        <b>Q-gym: An Equality Saturation Framework for DNN Inference Exploiting Weight Repetition  (#176)</b>

        <em>C. Fu, H. Huang, B. Wasti, C. Cummins, R. Baghdadi, K. Hazelwood, Y. Tian, J. Zhao, H. Leather</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    
  
      <tr>
        <td>12:00–13:30</td>
        <td>Lunch</td>
        <td><a href='attend.html#restaurants'>(Attendees on their own)</a></td>
      </tr>


    
  
    <tr>
      <td>13:30–15:00</td>
      <td>
  <p><b>Track 1: Getting more out of your GPU</b></p>
  
    <p><i>Session Chair: Perry Gibson</i></p>
  
  <ul>
    
      <li>
        13:30–14:00:
        <b>Locality-aware Optimizations for Improving Remote Memory Latency in Multi-GPU Systems  (#43)</b>

        <em>L. Belayneh, H. Ye, K. Chen, D. Blaauw, T. Mudge, R. Dreslinski, N. Talati</em>
      </li>
    
      <li>
        14:00–14:30:
        <b>GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the Cloud  (#50)</b>

        <em>X. Tan, P. Golikov, N. Vijaykumar, G. Pekhimenko</em>
      </li>
    
      <li>
        14:30–15:00:
        <b>NaviSim: A Highly Accurate GPU Simulator for AMD RDNA GPUs  (#135)</b>

        <em>Y. Bao, Y. Sun, Z. Feric, M. Shen, M. Weston, J. Abellán, T. Baruah, J. Kim, A. Joshi, D. Kaeli</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>13:30–15:00</td>
      <td>
  <p><b>Track 2: Better hardware</b></p>
  
    <p><i>Session Chair: Sushant Kondguli</i></p>
  
  <ul>
    
      <li>
        13:30–14:00:
        <b>mu-grind: A Framework for Dynamically Instrumenting HLS generated RTL  (#158)</b>

        <em>P. Vahdatnia, A. sharifian, R. Hojabr, A. Shriraman</em>
      </li>
    
      <li>
        14:00–14:30:
        <b>Athena: An Early-Fetch Architecture To Reduce On-Chip Page Walk Latencies  (#276)</b>

        <em>S. Ghahani, S. Khadirsharbiyani, J. Kotra, M. Kandemir</em>
      </li>
    
      <li>
        14:30–15:00:
        <b>DSDP: Dual Stream Data Prefetcher  (#204)</b>

        <em>M. He, H. Wang, K. Zhou, K. Cui, H. Yan, C. Guo, R. He</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    
  
      <tr>
        <td>15:00–15:30</td>
        <td>Coffee Break</td>
        <td>Discovery Room, DPI</td>
      </tr>


    
  
    <tr>
      <td>15:30–16:30</td>
      <td>
  <p><b>Track 1: Task parallelism</b></p>
  
    <p><i>Session Chair: Santosh Pande</i></p>
  
  <ul>
    
      <li>
        15:30–16:00:
        <b>Efficient task-mapping of parallel applications using a space-filling curve  (#83)</b>

        <em>O. Kwon, J. Kang, S. Lee, W. Kim, J. Song</em>
      </li>
    
      <li>
        16:00–16:30:
        <b>Auto-Partitioning Heterogeneous Task-Parallel Programs with StreamBlocks  (#103)</b>

        <em>M. Emami, E. Bezati, J. Janneck, J. Larus</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>15:30–16:30</td>
      <td>
  <p><b>Track 2: Optimization</b></p>
  
    <p><i>Session Chair: Nicolas Agostini</i></p>
  
  <ul>
    
      <li>
        15:30–16:00:
        <b>Optimizing Regular Expressions via Rewrite-Guided Synthesis  (#127)</b>

        <em>J. McClurg, M. Claver, J. Garner, J. Vossen, J. Schmerge, M. Belviranli</em>
      </li>
    
      <li>
        16:00–16:30:
        <b>Combining Run-time Checks and Compile-time Analysis to Improve Control Flow Auto-Vectorization  (#120)</b>

        <em>B. Liu, A. Laird, W. Tsang, B. Mahjour, M. Dehnavi</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    <tr>
      <td>16:30–17:00</td>
      <td><p>Travel to boat dock</p>
        The dock is a <b>30-minute walk from DPI</b>. Please make sure to allow sufficient time.
      </td>
      <td>(Attendees on their own)</td>
    </tr>
    
  
      <tr>
        <td>17:00–20:30</td>
        <td>Banquet / Excursion: Architecture Boat Tour (boarding starts 17:15, vessel departs 17:30 sharp)</td>
        <td><a href='https://www.google.com/maps/place/Wendella+tours+Docks+3+and+4/@41.8891462,-87.6278298,17z/data=!3m1!4b1!4m5!3m4!1s0x880e2d071c55a07d:0x4832841889121c35!8m2!3d41.8891462!4d-87.6256411'>Wendella West Dock 4</a></td>
      </tr>


  </table>


    <a id="keynote-1"></a>

    <h4>Keynote: MemComputing: Fundamentals and Applications</h4>
<p><strong>Massimiliano Di Ventra</strong> (Department of Physics, University of California San Diego, La Jolla, CA)</p>
<p>MemComputing is a new physics-based approach to computation
that employs <em>time non-locality</em> (memory) to both process and
store information on the same physical location. (M. Di Ventra,
<em>MemComputing: Fundamentals and Applications</em>, Oxford University
Press, 2022.) Its digital version is designed to solve combinatorial
optimization problems. A practical realization of digital memcomputing
machines (DMMs) can be accomplished via circuits of non-linear
dynamical systems with memory engineered so that periodic orbits and
chaos can be avoided. A given logic (or algebraic) problem is first
mapped into this type of dynamical system whose point attractors
represent the solutions of the original problem. A DMM then finds
the solution via a succession of elementary avalanches (instantons)
whose role is to eliminate configurations of logical inconsistency
("logical defects") from the circuit. I will discuss the physics
behind MemComputing and show many examples of its applicability
to various combinatorial optimization problems, Machine Learning,
and Quantum Mechanics, demonstrating its advantages over traditional
approaches and even quantum computing. Work supported by DARPA, DOE,
NSF, CMRR, and <a href="http://memcpu.com/">MemComputing, Inc</a>.</p>
<blockquote>
<p><img alt="Massimiliano Di Ventra" src="images-generated/headshot-diventra.jpg" /></p>
<p>Massimiliano Di Ventra obtained his undergraduate degree in Physics
<em>summa cum laude</em> from the University of Trieste (Italy) in 1991 and
did his PhD studies at the Swiss Federal Institute of Technology
in Lausanne in 1993-1997. He is now professor of Physics at the
University of California, San Diego. Di Ventra's research interests
are in condensed-matter theory and unconventional computing. He
has been invited to deliver more than 300 talks worldwide on these
topics. He has published more than 200 papers in refereed journals,
4 textbooks, and has 7 granted patents (3 foreign). He is a fellow
of the IEEE, the American Physical Society, the Institute of Physics,
and a foreign member of Academia Europaea. In 2018 he was named Highly
Cited Researcher by Clarivate Analytics, he is the recipient of the
2020 Feynman Prize for theory in Nanotechnology, and is a 2022 IEEE
Nanotechnology Council Distinguished Lecturer. He is the co-founder
of <a href="http://memcpu.com/">MemComputing, Inc</a>.</p>
</blockquote>

    <a id="main2"></a>

    <h2>Wednesday, October 12, 2022</h2>

    <a href="#top">Back to navigation</a>

    
  <table class="table table-striped">
    <thead>
      <td>Time</td>
      <td>What</td>
      <td>Where</td>
    </thead>
    
        
      <tr>
        <td>7:30</td>
        <td><p>Registration opens</p>

              <p>
              Please allow sufficient time to clear building security.
              See <a href='attend.html#arrival'>here</a> for instructions.
              </p>
              </td>
        <td>DPI</td>
      </tr>


    
  
      <tr>
        <td>7:30–8:30</td>
        <td>Continental Breakfast</td>
        <td>Discovery Room, DPI</td>
      </tr>


    
  <tr>
    <td>8:30–9:30</td>
    <td>Keynote: <b><a href="#keynote-2">AI Acceleration:  Co-optimizing Algorithms, Hardware, and Software</a></b></td>
    <td>Discovery Room, DPI</td>
  </tr>

    <tr>
      <td>9:30–10:30</td>
      <td>
        <p>Talks: <b>ACM SRC Finalists</b></p>
        <ul>
          <li><b>Understanding Correlated Error Events in Quantum Computers</b> <em>Michael Schleppy & Arpan Gupta</em> (undergrad)</li>
          <li><b>Independent Tenancy Model</b> <em>Boyang Wang</em> (undergrad)</li>
          <li><b>A GPU Acceleration Flow for Parallel RTL Simulation and Hardware Testing</b> <em>Dian-Lun Lin</em> (grad)</li>
          <li><b>SuperB-NoC: A Superconducting Buffering NoC</b> <em>Rhys Gretsch</em> (grad)</li>
          <li><b>Automatically Translating Non-Affine Codes</b> <em>Avery Laird</em> (grad)</li>
        </ul>
      </td>
      <td>Discovery Room, DPI</td>
    </tr>
    
  
      <tr>
        <td>10:30–11:00</td>
        <td>Coffee Break</td>
        <td>Discovery Room, DPI</td>
      </tr>


    
  
    <tr>
      <td>11:00–12:30</td>
      <td>
  <p><b>Track 1: GPU algorithms</b></p>
  
    <p><i>Session Chair: Jose Moreira</i></p>
  
  <ul>
    
      <li>
        11:00–11:30:
        <b>Parallelizing Neural Network Models Effectively on GPU by Implementing Reductions Atomically  (#78)</b>

        <em>J. Zhao, C. Bastoul, Y. Yi, J. Hu, W. Nie, R. Zhang, Z. Geng, C. Li, T. Tachon, Z. Gan</em>
      </li>
    
      <li>
        11:30–12:00:
        <b>GAP: GPU Adaptive In-situ Parallel Analytics  (#114)</b>

        <em>H. Xing, G. Agrawal, R. Ramnath</em>
      </li>
    
      <li>
        12:00–12:30:
        <b>A GPU Multiversion B-Tree  (#258)</b>

        <em>M. Awad, S. Porumbescu, J. Owens</em>
      </li>
    
  </ul>
</td>
      <td>Discovery Room, DPI</td>
    </tr>
  
    <tr>
      <td>11:00–12:30</td>
      <td>
  <p><b>Track 2: Portable performance</b></p>
  
    <p><i>Session Chair: P. Sadayappan</i></p>
  
  <ul>
    
      <li>
        11:00–11:30:
        <b>Breaking the Vendor Lock --- Performance Portable Programming Through OpenMP as Target Independent Runtime Layer  (#312)</b>

        <em>J. Doerfert, M. Jasper, J. Huber, K. Abdelaal, G. Georgakoudis, T. Scogland, K. Parasyris</em>
      </li>
    
      <li>
        11:30–12:00:
        <b>BenchPress: A Deep Active Benchmark Generator  (#10)</b>

        <em>F. Tsimpourlas, P. Petoumenos, M. Xu, C. Cummins, K. Hazelwood, A. Rajan, H. Leather</em>
      </li>
    
      <li>
        12:00–12:30:
        <b>Collage: Seamless Integration of Deep Learning Backends with Automatic Placement  (#52)</b>

        <em>B. Jeon, S. Park, P. Liao, S. Xu, T. Chen, Z. Jia</em>
      </li>
    
  </ul>
</td>
      <td>Orange & Blue Room, IC</td>
    </tr>
  

    <tr>
      <td>12:30–12:45</td>
      <td>Conference Closing</td>
      <td>Discovery Room, DPI</td>
    </tr>
  </table>


    <a id="keynote-2"></a>

    <h4>Keynote: AI Acceleration:  Co-optimizing Algorithms, Hardware, and Software</h4>
<p><strong>Vijayalakshmi Srinivasan</strong> (IBM Research, Yorktown Heights, NY)</p>
<p>The combination of growth in compute capabilities and availability
of large datasets has led to a re-birth of deep learning. Deep Neural
Networks (DNNs) have become state-of-the-art in a variety of machine
learning tasks spanning domains across vision, speech, and machine
translation. Deep Learning (DL) achieves high accuracy in these
tasks at the expense of 100s of ExaOps of computation. Hardware
specialization and acceleration is a key enabler to improve
operational efficiency of DNNs, in turn requiring synergistic
cross-layer design across algorithms, hardware, and software.</p>
<p>In this talk I will present this holistic approach adopted in the
design of a multi-TOPs AI hardware accelerator. Key advances in
the AI algorithm/application-level exploiting approximate computing
techniques enable deriving low-precision DNNs models that maintain
the same level of accuracy. Hardware performance-aware design space
exploration is critical during compilation to map DNNs with diverse
computational characteristics systematically and optimally while
preserving familiar programming and user interfaces. The opportunities
to co-optimize the algorithms, hardware, and the software provides
the roadmap to continue to deliver superior performance over the
next decade.</p>
<blockquote>
<p><img alt="Vijayalakshmi Srinivasan" src="images-generated/headshot-srinivasan.jpg" /></p>
<p>Viji Srinivasan is a Distinguished Research Staff Member and a
manager of the accelerator architectures and compilers group at
the IBM T.J. Watson Research Center in Yorktown Heights. At IBM,
she has worked on various aspects of data management including
energy-efficient processor designs, microarchitecture of the memory
hierarchies of large-scale servers, cache coherence management
of symmetric multiprocessors, accelerators for data analytics
applications and more recently end-to-end accelerator solutions for
AI. Many of her research contributions have been incorporated into
IBM's Power and System-z Enterprise-class servers.</p>
</blockquote>


    </div>
    <div class="col-md-4">
      <h2>Important Dates and Deadlines</h2>
<p><strong>Conference Papers:</strong></p>
<ul>
<li><del>Abstracts: April 18, 2022</del></li>
<li><strong><del>Full Papers: April 25, 2022</del></strong></li>
<li><del>Round 1 Rebuttal: June 6–9, 2022</del></li>
<li><del>Round 2 Rebuttal: July 11–14, 2022</del></li>
<li><del>Author Notification: July 29, 2022</del></li>
<li><del>Camera Ready Papers: August 26, 2022</del></li>
</ul>
<p><strong>Posters:</strong></p>
<ul>
<li><del>Poster Submission Deadline: September 1, 2022</del></li>
<li><del>Author Notification: September 15, 2022</del></li>
<li><del>Extended Abstract: September 29, 2022</del></li>
<li><del>Poster Session: October 10, 2022</del></li>
</ul>
<p><strong><a href="./src.html">ACM Student Research Competition</a>:</strong></p>
<ul>
<li><del>Abstract Submission Deadline: September 8, 2022</del></li>
<li><del>Author Notification: September 16, 2022</del></li>
<li><del>SRC Poster Session: October 11, 2022</del></li>
<li><del>SRC Finalist Presentations: October 12, 2022</del></li>
</ul>
<p><strong><a href="attend.html#travel-awards">Student Travel Awards</a>:</strong></p>
<ul>
<li><del>Application Deadline: October 5, 2022</del></li>
</ul>
<p><strong>Workshops and Tutorials:</strong></p>
<ul>
<li>Workshops/Tutorials: October 8–9, 2022</li>
</ul>
<p><strong>Conference: October 10–12, 2022</strong></p>
<hr />
<h3>Previous PACTs</h3>
<ul>
<li><a href="http://pact21.snu.ac.kr/">PACT21</a>
  (<a href="https://ieeexplore.ieee.org/xpl/conhome/9563009/proceeding">proceedings</a>)</li>
<li><a href="https://pact20.cc.gatech.edu/">PACT20</a>
  (<a href="https://dl.acm.org/doi/proceedings/10.1145/3410463">proceedings</a>)</li>
<li><a href="https://hpc.pnl.gov/pact19/">PACT19</a>
  (<a href="https://ieeexplore.ieee.org/xpl/conhome/8890589/proceeding">proceedings</a>)</li>
<li><a href="http://www.cs.ucy.ac.cy/conferences/pact2018/">PACT18</a>
  (<a href="https://dl.acm.org/doi/proceedings/10.1145/3243176">proceedings</a>)</li>
<li><a href="https://parasol.tamu.edu/pact17/">PACT17</a>
  (<a href="https://www.computer.org/csdl/proceedings/pact/2017/12OmNwbcJ4M">proceedings</a>)</li>
<li>PACT16
  (<a href="https://dl.acm.org/doi/proceedings/10.1145/2967938">proceedings</a>),</li>
<li><a href="https://sites.google.com/a/lbl.gov/pact2015/">PACT15</a>
  (<a href="https://ieeexplore.ieee.org/xpl/conhome/7425426/proceeding">proceedings</a>)</li>
<li><a href="http://pact2014.pactconf.org/">PACT14</a>
  (<a href="https://dl.acm.org/doi/proceedings/10.1145/2628071">proceedings</a>)</li>
</ul>
<details>
<summary>Earlier PACTs</summary>
<ul>
<li><a href="http://conferences.inf.ed.ac.uk/pact2013/">PACT13</a></li>
<li><a href="http://pact2012.ece.northwestern.edu/index.html">PACT12</a></li>
<li><a href="https://parasol.tamu.edu/pact11/">PACT11</a></li>
<li><a href="http://www.complang.tuwien.ac.at/PACT-2010/">PACT10</a></li>
<li><a href="http://pact09.renci.org/">PACT09</a></li>
<li><a href="http://www.eecg.toronto.edu/pact/">PACT08</a></li>
<li><a href="http://pact07.cs.tamu.edu/">PACT07</a></li>
<li><a href="http://www.cs.virginia.edu/~pact2006/">PACT06</a></li>
<li><a href="http://pact05.ce.ucsc.edu/">PACT05</a></li>
<li><a href="https://home.mis.u-picardie.fr/~cerin/pact/">PACT04</a></li>
<li><a href="https://www.ccis.northeastern.edu/pact03/">PACT03</a></li>
<li><a href="http://moss.csc.ncsu.edu/pact02/">PACT02</a></li>
<li><a href="http://research.ac.upc.es/pact01/">PACT01</a></li>
<li><a href="http://eecs.oregonstate.edu/pact2000/">PACT00</a></li>
<li><a href="http://www.cs.ucy.ac.cy/~pact99/">PACT99</a></li>
</ul>
</details>
<hr />
<h3>Sponsors</h3>
<h4>Platinum</h4>
<p><img alt="Qualcomm" src="images-generated/qualcomm.png" /></p>
<h4>Gold</h4>
<p><img alt="Huawei" src="images-generated/huawei.png" /></p>
<h4>Supporters</h4>
<p><img alt="ACM SIGARCH" src="images/acm-sigarch-logo.png" /></p>
<p><img alt="IEEE Computer Society" src="images/ieee-cs-logo.png" /></p>
<div class="text-muted">
<ul>
<li><a href="https://tc.computer.org/tcpp/">Technical Committee on Parallel Processing</a></li>
<li><a href="https://ieeetcca.org/">Technical Committee on Computer Architecture</a></li>
</ul>
</div>
<p><img alt="IFIP" src="images/ifip-logo.png" /></p>
<h4>Academic</h4>
<p><a href="https://cs.illinois.edu"><img alt="CS@Illinois" src="images-generated/illinois-wordmark.png" /></a></p>
    </div>
  </div>

      </main>
      <footer class="pt-5 my-5 text-muted border-top">
        © PACT 2022 Organizers
        ·
        <a href="https://github.com/pact-2022/pact-2022.github.io">
          Edit or suggest changes to this web site</a>
      </footer>
    </div>
  </body>
</html>
